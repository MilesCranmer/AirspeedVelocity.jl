<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · AirspeedVelocity.jl</title><meta name="title" content="Home · AirspeedVelocity.jl"/><meta property="og:title" content="Home · AirspeedVelocity.jl"/><meta property="twitter:title" content="Home · AirspeedVelocity.jl"/><meta name="description" content="Documentation for AirspeedVelocity.jl."/><meta property="og:description" content="Documentation for AirspeedVelocity.jl."/><meta property="twitter:description" content="Documentation for AirspeedVelocity.jl."/><meta property="og:url" content="https://MilesCranmer.github.io/AirspeedVelocity.jl/"/><meta property="twitter:url" content="https://MilesCranmer.github.io/AirspeedVelocity.jl/"/><link rel="canonical" href="https://MilesCranmer.github.io/AirspeedVelocity.jl/"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>AirspeedVelocity.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li><li><a class="tocitem" href="#Using-in-CI"><span>Using in CI</span></a></li><li><a class="tocitem" href="#Further-examples"><span>Further examples</span></a></li><li><a class="tocitem" href="#CLI-Reference"><span>CLI Reference</span></a></li><li><a class="tocitem" href="#Related-packages"><span>Related packages</span></a></li></ul></li><li><a class="tocitem" href="api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/MilesCranmer/AirspeedVelocity.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/MilesCranmer/AirspeedVelocity.jl/blob/master/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="AirspeedVelocity.jl"><a class="docs-heading-anchor" href="#AirspeedVelocity.jl">AirspeedVelocity.jl</a><a id="AirspeedVelocity.jl-1"></a><a class="docs-heading-anchor-permalink" href="#AirspeedVelocity.jl" title="Permalink"></a></h1><p><a href="https://MilesCranmer.github.io/AirspeedVelocity.jl/stable/"><img src="https://img.shields.io/badge/docs-stable-blue.svg" alt="Stable"/></a> <a href="https://MilesCranmer.github.io/AirspeedVelocity.jl/dev/"><img src="https://img.shields.io/badge/docs-dev-blue.svg" alt="Dev"/></a> <a href="https://github.com/MilesCranmer/AirspeedVelocity.jl/actions/workflows/CI.yml?query=branch%3Amaster"><img src="https://github.com/MilesCranmer/AirspeedVelocity.jl/actions/workflows/CI.yml/badge.svg?branch=master" alt="Build Status"/></a> <a href="https://coveralls.io/github/MilesCranmer/AirspeedVelocity.jl?branch=master"><img src="https://coveralls.io/repos/github/MilesCranmer/AirspeedVelocity.jl/badge.svg?branch=master" alt="Coverage"/></a></p><p>AirspeedVelocity.jl strives to make it easy to benchmark Julia packages over their lifetime. It is inspired by <a href="https://asv.readthedocs.io/en/stable/">asv</a>.</p><p>This package allows you to:</p><ul><li>Generate benchmarks directly from the terminal with an easy-to-use CLI.</li><li>Compare many commits/tags/branches at once.</li><li>Plot those benchmarks, automatically flattening your benchmark suite into a list of plots with generated titles.</li><li>Run in CI with a one‑line GitHub Action that comments benchmark results on every PR.</li></ul><p>This package also freezes the benchmark script at a particular revision, so there is no worry about the old history overwriting the benchmark.</p><p>https://github.com/MilesCranmer/AirspeedVelocity.jl/assets/7593028/f27b04ef-8491-4f49-a312-4df0fae00598</p><ul><li><a href="#airspeedvelocityjl">AirspeedVelocity.jl</a><ul><li><a href="#installation">Installation</a></li><li><a href="#examples">Examples</a></li><li><a href="#using-in-ci">Using in CI</a><ul><li><a href="#option-1-pr-comments">Option 1: PR Comments</a></li><li><a href="#option-2-job-summary">Option 2: Job Summary</a></li><li><a href="#multiple-julia-versions">Multiple Julia versions</a></li><li><a href="#ci-parameters">CI Parameters</a></li></ul></li><li><a href="#further-examples">Further examples</a></li><li><a href="#cli-reference">CLI Reference</a><ul><li><a href="#benchpkg"><code>benchpkg</code></a></li><li><a href="#benchpkgtable"><code>benchpkgtable</code></a></li><li><a href="#benchpkgplot"><code>benchpkgplot</code></a></li></ul></li><li><a href="#related-packages">Related packages</a></li></ul></li></ul><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>You can install the CLI with:</p><pre><code class="language-bash hljs">julia -e &#39;using Pkg; Pkg.activate(temp=true); Pkg.add(&quot;AirspeedVelocity&quot;); Pkg.build(&quot;AirspeedVelocity&quot;)&#39;</code></pre><p>This will install two executables at <code>~/.julia/bin</code> - make sure to have it on your <code>PATH</code>.</p><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><p>You may use the CLI to generate benchmarks for any package with, e.g.,</p><pre><code class="language-bash hljs">benchpkg</code></pre><p>This will benchmark the package defined in the current directory at the current dirty state, against the default branch (i.e., <code>main</code> or <code>master</code>), over all benchmarks defined in <code>benchmark/benchmarks.jl</code> using BenchmarkTools.jl. You should have a <code>const SUITE = BenchmarkGroup()</code> defined in this file, which you have added benchmarks to.</p><p>This will then print a markdown table of the results while also saving the JSON results to the current directory.</p><p>See the <a href="#further-examples">further examples</a> for more details.</p><h2 id="Using-in-CI"><a class="docs-heading-anchor" href="#Using-in-CI">Using in CI</a><a id="Using-in-CI-1"></a><a class="docs-heading-anchor-permalink" href="#Using-in-CI" title="Permalink"></a></h2><p>AirspeedVelocity.jl provides two ways to display benchmark results in GitHub Actions:</p><h3 id="Option-1:-PR-Comments"><a class="docs-heading-anchor" href="#Option-1:-PR-Comments">Option 1: PR Comments</a><a id="Option-1:-PR-Comments-1"></a><a class="docs-heading-anchor-permalink" href="#Option-1:-PR-Comments" title="Permalink"></a></h3><p>Posts benchmark results as comments on pull requests.</p><p>Add <code>.github/workflows/benchmark.yml</code> to your package:</p><pre><code class="language-yaml hljs">name: Benchmark this PR
on:
  pull_request_target:
    branches: [ master ]  # change to your default branch
permissions:
  pull-requests: write    # needed to post comments

jobs:
  bench:
    runs-on: ubuntu-latest
    steps:
      - uses: MilesCranmer/AirspeedVelocity.jl@action-v1
        with:
          julia-version: &#39;1&#39;</code></pre><h3 id="Option-2:-Job-Summary"><a class="docs-heading-anchor" href="#Option-2:-Job-Summary">Option 2: Job Summary</a><a id="Option-2:-Job-Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Option-2:-Job-Summary" title="Permalink"></a></h3><p>Displays benchmark results in the GitHub Actions job summary (visible in the Actions tab).</p><pre><code class="language-yaml hljs">name: Benchmark this PR
on:
  pull_request:             # no need for pull_request_target
    branches: [ master ]
# no permissions needed

jobs:
  bench:
    runs-on: ubuntu-latest
    steps:
      - uses: MilesCranmer/AirspeedVelocity.jl@action-v1
        with:
          julia-version: &#39;1&#39;
          job-summary: &#39;true&#39;</code></pre><p>Both workflows run AirspeedVelocity and display results with separate, collapsible tables for runtime and memory.</p><h3 id="Multiple-Julia-versions"><a class="docs-heading-anchor" href="#Multiple-Julia-versions">Multiple Julia versions</a><a id="Multiple-Julia-versions-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Julia-versions" title="Permalink"></a></h3><pre><code class="language-yaml hljs">strategy:
  matrix:
    julia: [&#39;1&#39;, &#39;1.10&#39;]

steps:
  - uses: MilesCranmer/AirspeedVelocity.jl@action-v1
    with:
      julia-version: ${{ matrix.julia }}</code></pre><p>Each matrix leg writes its own comment (Option 1) or section in the job summary (Option 2).</p><h3 id="CI-Parameters"><a class="docs-heading-anchor" href="#CI-Parameters">CI Parameters</a><a id="CI-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#CI-Parameters" title="Permalink"></a></h3><table><tr><th style="text-align: right">Input</th><th style="text-align: right">Default</th><th style="text-align: right">What it does</th></tr><tr><td style="text-align: right"><code>asv-version</code></td><td style="text-align: right"><code>&quot;0.6&quot;</code></td><td style="text-align: right">AirspeedVelocity version to install</td></tr><tr><td style="text-align: right"><code>julia-version</code></td><td style="text-align: right"><code>&quot;1&quot;</code></td><td style="text-align: right">Julia version to install</td></tr><tr><td style="text-align: right"><code>job-summary</code></td><td style="text-align: right"><code>&quot;false&quot;</code></td><td style="text-align: right">Output to job summary instead of PR comment</td></tr><tr><td style="text-align: right"><code>tune</code></td><td style="text-align: right"><code>&quot;false&quot;</code></td><td style="text-align: right"><code>--tune</code> to tune benchmarks first</td></tr><tr><td style="text-align: right"><code>mode</code></td><td style="text-align: right"><code>&quot;time,memory&quot;</code></td><td style="text-align: right">Which tables to generate (<code>time</code>, <code>memory</code>)</td></tr><tr><td style="text-align: right"><code>enable-plots</code></td><td style="text-align: right"><code>&quot;false&quot;</code></td><td style="text-align: right">Upload PNG plots as artifact</td></tr><tr><td style="text-align: right"><code>script</code></td><td style="text-align: right"><code>&quot;&quot;</code></td><td style="text-align: right">Custom benchmark script path</td></tr><tr><td style="text-align: right"><code>rev</code></td><td style="text-align: right"><code>&quot;&quot;</code></td><td style="text-align: right"><code>--rev</code> list for benchpkg (comma-separated)</td></tr><tr><td style="text-align: right"><code>bench-on</code></td><td style="text-align: right"><code>&quot;&quot;</code></td><td style="text-align: right"><code>--bench-on</code> commit to freeze script</td></tr><tr><td style="text-align: right"><code>filter</code></td><td style="text-align: right"><code>&quot;&quot;</code></td><td style="text-align: right"><code>--filter</code> list for <code>benchpkg</code></td></tr><tr><td style="text-align: right"><code>exeflags</code></td><td style="text-align: right"><code>&quot;&quot;</code></td><td style="text-align: right"><code>--exeflags</code> for Julia runner</td></tr><tr><td style="text-align: right"><code>extra-pkgs</code></td><td style="text-align: right"><code>&quot;&quot;</code></td><td style="text-align: right"><code>--add</code> extra packages (comma-separated)</td></tr></table><h2 id="Further-examples"><a class="docs-heading-anchor" href="#Further-examples">Further examples</a><a id="Further-examples-1"></a><a class="docs-heading-anchor-permalink" href="#Further-examples" title="Permalink"></a></h2><p>You can configure all options with the CLI flags. For example, to benchmark the registered package <code>Transducers.jl</code> at the revisions <code>v0.4.20</code>, <code>v0.4.70</code>, and <code>master</code>, you can use:</p><pre><code class="language-bash hljs">benchpkg Transducers \
    --rev=v0.4.20,v0.4.70,master \
    --bench-on=v0.4.20</code></pre><p>This will further use the benchmark script <code>benchmark/benchmarks.jl</code> as it was defined at <code>v0.4.20</code>, and then save the JSON results in the current directory.</p><p>We can explicitly view the results of the benchmark as a table with <code>benchpkgtable</code>:</p><pre><code class="language-bash hljs">benchpkgtable Transducers \
    --rev=v0.4.20,v0.4.70,master</code></pre><p>We can also generate plots of the revisions with:</p><pre><code class="language-bash hljs">benchpkgplot Transducers \
    --rev=v0.4.20,v0.4.70,master \
    --format=pdf \
    --npart=5</code></pre><p>which will generate a pdf file for each set of 5 plots, showing the change with each revision:</p><p><img src="https://user-images.githubusercontent.com/7593028/229543368-14b1da88-8315-437b-b38f-fff143f26e3a.png" alt="runtime_at_versions"/></p><p>You can also provide a custom benchmark. For example, let&#39;s say you have a file <code>script.jl</code>, defining a benchmark for <code>SymbolicRegression.jl</code> (we always need to define the <code>SUITE</code> variable as a <code>BenchmarkGroup</code>):</p><pre><code class="language-julia hljs">using BenchmarkTools, SymbolicRegression
const SUITE = BenchmarkGroup()

# Create hierarchy of benchmarks:
SUITE[&quot;eval_tree_array&quot;] = BenchmarkGroup()

options = Options(; binary_operators=[+, -, *], unary_operators=[cos])
tree = Node(; feature=1) + cos(3.2f0 * Node(; feature=2))


for n in [10, 20]
    SUITE[&quot;eval_tree_array&quot;][n] = @benchmarkable(
        eval_tree_array($tree, X, $options),
        evals=10,
        samples=1000,
        setup=(X=randn(Float32, 2, $n))
    )
end
</code></pre><p>Inside this script, we will also have access to the <code>PACKAGE_VERSION</code> constant, to allow for different behavior depending on tag. We can run this benchmark over the history of <code>SymbolicRegression.jl</code> with:</p><pre><code class="language-bash hljs">benchpkg SymbolicRegression \
    -r v0.15.3,v0.16.2 \
    -s script.jl \
    -o results/ \
    --exeflags=&quot;--threads=4 -O3&quot;</code></pre><p>where we have also specified the output directory and extra flags to pass to the <code>julia</code> executable. We can also now visualize this:</p><pre><code class="language-bash hljs">benchpkgplot SymbolicRegression \
    -r v0.15.3,v0.16.2 \
    -i results/ \
    -o plots/</code></pre><h2 id="CLI-Reference"><a class="docs-heading-anchor" href="#CLI-Reference">CLI Reference</a><a id="CLI-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#CLI-Reference" title="Permalink"></a></h2><h3 id="benchpkg"><a class="docs-heading-anchor" href="#benchpkg"><code>benchpkg</code></a><a id="benchpkg-1"></a><a class="docs-heading-anchor-permalink" href="#benchpkg" title="Permalink"></a></h3><p>For running benchmarks, you can use the <code>benchpkg</code> command, which is built into the <code>~/.julia/bin</code> folder:</p><pre><code class="language-markdown hljs">    benchpkg [package_name] [-r --rev &lt;arg&gt;]
                            [--url &lt;arg&gt;]
                            [--path &lt;arg&gt;]
                            [-o, --output-dir &lt;arg&gt;]
                            [-e, --exeflags &lt;arg&gt;]
                            [-a, --add &lt;arg&gt;]
                            [-s, --script &lt;arg&gt;]
                            [--bench-on &lt;arg&gt;]
                            [-f, --filter &lt;arg&gt;]
                            [--nsamples-load-time &lt;arg&gt;]
                            [--tune]
                            [--dont-print]

Benchmark a package over a set of revisions.

#### Arguments

- `package_name`: Name of the package. If not given, the package is assumed to be
  the current directory.

#### Options

- `-r, --rev &lt;arg&gt;`: Revisions to test (delimit by comma). Use `dirty` to
  benchmark the current state of the package at `path` (and not a git commit).
  The default is `{DEFAULT},dirty`, which will attempt to find the default branch
  of the package.
- `--url &lt;arg&gt;`: URL of the package.
- `--path &lt;arg&gt;`: Path of the package. The default is `.` if other arguments are not given.
- `-o, --output-dir &lt;arg&gt;`: Where to save the JSON results. The default is `.`.
- `-e, --exeflags &lt;arg&gt;`: CLI flags for Julia (default: none).
- `-a, --add &lt;arg&gt;`: Extra packages needed (delimit by comma).
- `-s, --script &lt;arg&gt;`: The benchmark script. Default: `benchmark/benchmarks.jl` downloaded from `stable`.
- `--bench-on &lt;arg&gt;`: If the script is not set, this specifies the revision at which
  to download `benchmark/benchmarks.jl` from the package.
- `-f, --filter &lt;arg&gt;`: Filter the benchmarks to run (delimit by comma).
- `--nsamples-load-time &lt;arg&gt;`: Number of samples to take when measuring load time of
    the package (default: 5). (This means starting a Julia process for each sample.)
- `--dont-print`: Don&#39;t print the table.

#### Flags

- `--tune`: Whether to run benchmarks with tuning (default: false).</code></pre><h3 id="benchpkgtable"><a class="docs-heading-anchor" href="#benchpkgtable"><code>benchpkgtable</code></a><a id="benchpkgtable-1"></a><a class="docs-heading-anchor-permalink" href="#benchpkgtable" title="Permalink"></a></h3><p>You can also just generate a table from stored JSON results:</p><pre><code class="language-markdown hljs">    benchpkgtable [package_name] [-r --rev &lt;arg&gt;]
                                 [-i --input-dir &lt;arg&gt;]
                                 [--ratio]
                                 [--mode &lt;arg&gt;]
                                 [--url &lt;arg&gt;]
                                 [--path &lt;arg&gt;]

Print a table of the benchmarks of a package as created with `benchpkg`.

#### Arguments

- `package_name`: Name of the package.

#### Options

- `-r, --rev &lt;arg&gt;`: Revisions to test (delimit by comma).
  The default is `{DEFAULT},dirty`, which will attempt to find the default branch
  of the package.
- `-i, --input-dir &lt;arg&gt;`: Where the JSON results were saved (default: &quot;.&quot;).
- `--url &lt;arg&gt;`: URL of the package. Only used to get the package name.
- `--path &lt;arg&gt;`: Path of the package. The default is `.` if other arguments are not given.
   Only used to get the package name.

#### Flags

- `--ratio`: Whether to include the ratio (default: false). Only applies when
    comparing two revisions.
- `--mode`: Table mode(s). Valid values are &quot;time&quot; (default), to print the
    benchmark time, or &quot;memory&quot;, to print the allocation and memory usage.
    Both options can be passed, if delimited by comma.</code></pre><h3 id="benchpkgplot"><a class="docs-heading-anchor" href="#benchpkgplot"><code>benchpkgplot</code></a><a id="benchpkgplot-1"></a><a class="docs-heading-anchor-permalink" href="#benchpkgplot" title="Permalink"></a></h3><p>For plotting, you can use the <code>benchpkgplot</code> function:</p><pre><code class="language-markdown hljs">    benchpkgplot package_name [-r --rev &lt;arg&gt;]
                              [-i --input-dir &lt;arg&gt;]
                              [-o --output-dir &lt;arg&gt;]
                              [-n --npart &lt;arg&gt;]
                              [--format &lt;arg&gt;]

Plot the benchmarks of a package as created with `benchpkg`.

#### Arguments

- `package_name`: Name of the package.

#### Options

- `-r, --rev &lt;arg&gt;`: Revisions to test (delimit by comma).
- `-i, --input-dir &lt;arg&gt;`: Where the JSON results were saved (default: &quot;.&quot;).
- `-o, --output-dir &lt;arg&gt;`: Where to save the plots results (default: &quot;.&quot;).
- `-n, --npart &lt;arg&gt;`: Max number of plots per page (default: 10).
- `--format &lt;arg&gt;`: File type to save the plots as (default: &quot;png&quot;).</code></pre><p>If you prefer to use the Julia API, you can use the <code>benchmark</code> function for generating data. The API is given <a href="https://astroautomata.com/AirspeedVelocity.jl/dev/api/">here</a>.</p><h2 id="Related-packages"><a class="docs-heading-anchor" href="#Related-packages">Related packages</a><a id="Related-packages-1"></a><a class="docs-heading-anchor-permalink" href="#Related-packages" title="Permalink"></a></h2><p>Also be sure to check out <a href="https://github.com/JuliaCI/PkgBenchmark.jl">PkgBenchmark.jl</a>. PkgBenchmark.jl is a simple wrapper of BenchmarkTools.jl to interface it with Git, and is a good choice for building custom analysis workflows.</p><p>However, for me this wrapper is a bit too thin, which is why I created this package. AirspeedVelocity.jl tries to have more features and workflows readily-available. It also emphasizes a CLI (though there is a Julia API), as my subjective view is that this is more suitable for interacting side-by-side with <code>git</code>.</p><ul><li><a href="api/#AirspeedVelocity.BenchPkg.benchpkg"><code>AirspeedVelocity.BenchPkg.benchpkg</code></a></li><li><a href="api/#AirspeedVelocity.BenchPkgPlot.benchpkgplot"><code>AirspeedVelocity.BenchPkgPlot.benchpkgplot</code></a></li><li><a href="api/#AirspeedVelocity.BenchPkgTable.benchpkgtable"><code>AirspeedVelocity.BenchPkgTable.benchpkgtable</code></a></li><li><a href="api/#AirspeedVelocity.PlotUtils.combined_plots-Tuple{OrderedDict}"><code>AirspeedVelocity.PlotUtils.combined_plots</code></a></li><li><a href="api/#AirspeedVelocity.TableUtils.create_table-Tuple{OrderedDict}"><code>AirspeedVelocity.TableUtils.create_table</code></a></li><li><a href="api/#AirspeedVelocity.Utils.benchmark-Tuple{Vector{PackageSpec}}"><code>AirspeedVelocity.Utils.benchmark</code></a></li><li><a href="api/#AirspeedVelocity.Utils.benchmark-Tuple{String, Vector{String}}"><code>AirspeedVelocity.Utils.benchmark</code></a></li><li><a href="api/#AirspeedVelocity.Utils.load_results-Tuple{Vector{PackageSpec}}"><code>AirspeedVelocity.Utils.load_results</code></a></li></ul></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="api/">API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Sunday 19 October 2025 08:33">Sunday 19 October 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
